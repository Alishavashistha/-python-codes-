{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\ecb\\\\Downloads\\\\polynomial_data.csv\")\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df[\"x_train\"], df[\"y_train\"], color=\"teal\", label=\"Loaded Data\", alpha=0.8)\n",
    "plt.title(\"Loaded Polynomial Regression Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Loaded data shape:\", df.shape)\n",
    "\n",
    "#implementing polynomial regression\n",
    "\n",
    "# Extract training data columns from the DataFrame\n",
    "x_train = df[\"x_train\"]\n",
    "y_train = df[\"y_train\"]\n",
    "\n",
    "# Convert pandas series to NumPy arrays and reshape to column vectors\n",
    "x1 = x_train.to_numpy().reshape(-1, 1)\n",
    "Y = y_train.to_numpy().reshape(-1, 1)\n",
    "\n",
    "#normalizing\n",
    "mean = np.mean(x1)\n",
    "std = np.std(x1)\n",
    "x1 = (x1 - mean) / std\n",
    "\n",
    "# Initialize weights \n",
    "w = np.array([[1.0],[2.9], [1.0], [0.5],[1.0]])\n",
    "\n",
    "# Initialize bias term\n",
    "b = 0\n",
    "\n",
    "all_cost = []   # List to store cost after every iteration\n",
    "m = x1.shape[0]   # Number of training examples\n",
    "lr = 0.03        # Learning rate\n",
    "\n",
    "# Generate polynomial features (x² to x⁵)\n",
    "x2 = x1**2 \n",
    "x3 = x1**3\n",
    "x4 = x1**4\n",
    "x5 = x1**5\n",
    "\n",
    "X = np.concatenate([x1, x2, x3, x4, x5], axis=1)    # Concatenate all polynomial features\n",
    "print(X.shape)\n",
    "for _ in range(6000):     # Gradient Descent Loop for 6000 iterations\n",
    "    y_ = X @ w + b           # Forward pass: compute predictions\n",
    "    j = (1/2*m) * np.sum((y_ - Y)**2)     # Compute Mean Squared Error cost function\n",
    "    all_cost.append(j)       # Store the cost for visualization later\n",
    "\n",
    "    #update weights and bias\n",
    "    w = w - lr * (1/m) * X.T @ (y_ - Y) \n",
    "    b = b - lr * (1/m) * np.sum((y_ - Y))\n",
    "\n",
    " # Plot cost function vs iteration to visualize convergence\n",
    "plt.scatter(x1, Y)\n",
    "plt.plot(x1, y_, color='red')\n",
    "plt.show()\n",
    "plt.plot(all_cost)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
